{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6c867e37-9db0-4d65-80de-3feee033aa4f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Welcome to the Spark RAPIDS NDS Demo Lab\n",
    "This notebook will guide you through understanding how to run GPU [NDS queries](https://github.com/NVIDIA/spark-rapids-benchmarks/tree/dev/nds) using Spark RAPIDS.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b85831-81fe-443b-916e-596153fb6c24",
   "metadata": {
    "tags": []
   },
   "source": [
    "## View GPU configs\n",
    "To view the configurations used for the Spark RAPIDS job using GPU resources, you can see the various Spark settings that will be used in the job (and later tuned).  Many of the settings are what would be default, but are shown here for completeness.\n",
    "\n",
    "Note that settings with the prefix \"spark.rapids\" are specific to using the Spark RAPIDS plugin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd2d3db-3386-474a-b1c5-66ab8b4124e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script env bash\n",
    "cd /tmp\n",
    "cat <<EOT > power_run_gpu.template\n",
    "\n",
    "export SPARK_HOME=${SPARK_HOME:-/usr/lib/spark}\n",
    "export SPARK_RAPIDS_PLUGIN_JAR=../../rapids-4-spark_2.12-23.12.1-cuda11.jar\n",
    "export NDS_LISTENER_JAR=${NDS_LISTENER_JAR:-./jvm_listener/target/nds-benchmark-listener-1.0-SNAPSHOT.jar}\n",
    "export PYTHONPATH=$SPARK_HOME/python:`echo $SPARK_HOME/python/lib/py4j-*.zip`\n",
    "\n",
    "export SPARK_CONF=(\"--conf\" \"spark.plugins=com.nvidia.spark.SQLPlugin\"\n",
    "                   \"--conf\" \"spark.sql.files.maxPartitionBytes=128mb\"\n",
    "                   \"--conf\" \"spark.rapids.sql.concurrentGpuTasks=1\"\n",
    "                   \"--conf\" \"spark.eventLog.enabled=true\"\n",
    "                   \"--conf\" \"spark.eventLog.dir=/tmp/spark-events\"\n",
    "                   \"--conf\" \"spark.hadoop.fs.s3a.impl=org.apache.hadoop.fs.s3a.S3AFileSystem\"\n",
    "                   \"--conf\" \"spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.AnonymousAWSCredentialsProvider\"\n",
    "                   \"--conf\" \"spark.jars.packages=org.apache.hadoop:hadoop-aws:3.2.2,com.amazonaws:aws-java-sdk-bundle:1.12.180\"\n",
    "                   \"--files\" \"\\$SPARK_HOME/examples/src/main/scripts/getGpusResources.sh\"\n",
    "                   \"--jars\" \"\\$SPARK_RAPIDS_PLUGIN_JAR,\\$NDS_LISTENER_JAR\")\n",
    "EOT"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d846082-e060-4cd1-a3a0-1286cd3b7455",
   "metadata": {},
   "source": [
    "## View SQL query\n",
    "You can view the SQL query syntax that will be executed as a Spark RAPIDS application using GPU resources.  Note that the queries are identical whether running on CPU or using Spark RAPIDS on GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f78d0ff-7c39-4b9a-890e-0b3804104499",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "query=\"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abb391f-9184-4136-8e24-08621fbe1028",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%script env query=\"$query\" bash\n",
    "cat ./query_files/query$query.sql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1586669-c79a-4a71-9c85-fcb52c977459",
   "metadata": {},
   "source": [
    "## Run GPU application\n",
    "The SQL query will be executed as a Spark RAPIDS job on your instance.  It will read data from S3 at a scale factor of 100GB and compute the query using GPU resources as applicable.  Once the query completes, it will output the entire duration for the run, including the table setup time as well as the individual query execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6302c76a-2a73-44f0-92c2-683d26722c7f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%script env query=\"$query\" bash\n",
    "cd ./spark-rapids-benchmarks/nds\n",
    "./spark-submit-template /tmp/power_run_gpu.template \\\n",
    "    nds_power.py \\\n",
    "    s3a://dli-public-datasets-us-west-2/production/x-ds-04-v1/parquet_sf100 \\\n",
    "    ../../query_files/query$query.sql \\\n",
    "    gpu-time-tuning-$query.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7fdbf0-58ba-44d3-9f07-0d25aec4579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "gpu_original_times = pd.read_csv(\"/dli/task/spark-rapids-benchmarks/nds/gpu-time-\" + query + \".csv\")\n",
    "gpu_tuning_times = pd.read_csv(\"/dli/task/spark-rapids-benchmarks/nds/gpu-time-tuning-\" + query + \".csv\")\n",
    "\n",
    "original_total_time = gpu_original_times[gpu_original_times['query'] == \"Total Time\"]['time/milliseconds'].values[0]\n",
    "tuning_total_time = gpu_tuning_times[gpu_tuning_times['query'] == \"Total Time\"]['time/milliseconds'].values[0]\n",
    "tuning_total_speedup = original_total_time / tuning_total_time\n",
    "\n",
    "print(\"================================\")\n",
    "print(\"Query \" + query + \" Total Speedup Results\")\n",
    "print(\"================================\")\n",
    "print(\"Original Total Time = \" + str(original_total_time))\n",
    "print(\"Tuning Total Time = \" + str(tuning_total_time))\n",
    "print(\"Acceleration = \" + str(round((tuning_total_speedup),2)))\n",
    "\n",
    "query_name = \"query\" + query\n",
    "original_query_time = gpu_original_times[gpu_original_times['query'] == query_name]['time/milliseconds'].values[0]\n",
    "tuning_query_time = gpu_tuning_times[gpu_tuning_times['query'] == query_name]['time/milliseconds'].values[0]\n",
    "tuning_query_speedup = original_query_time / tuning_query_time\n",
    "\n",
    "print(\"\")\n",
    "print(\"================================\")\n",
    "print(\"Query \" + query + \" SQL Speedup Results\")\n",
    "print(\"================================\")\n",
    "print(\"Original Query Time = \" + str(original_query_time))\n",
    "print(\"Tuning Query Time = \" + str(tuning_query_time))\n",
    "print(\"Acceleration = \" + str(round((tuning_query_speedup),2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69df7517-a47a-4cf0-8ce8-ad70eb41323c",
   "metadata": {},
   "source": [
    "The actual acceleration will depend on the specific hardware, but a 2x+ acceleration can be expected based on the previous analysis. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
